import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import joblib
from surprise import Dataset, Reader

# ‚îÄ‚îÄ‚îÄ Read data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
movies  = pd.read_csv("data/movies.csv")
ratings = pd.read_csv("data/ratings.csv")

# ‚îÄ‚îÄ‚îÄ Load our Models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with open("data/knn_item_model.pkl", "rb") as f:
    knn_item_model = pickle.load(f)
with open("data/knn_user_model.pkl", "rb") as f:
    knn_user_model = pickle.load(f)

# ‚îÄ‚îÄ‚îÄ Streamlit Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(page_title="Movie Recommendation System", layout="wide")
st.title("üé¨ Movie Recommendation System Presentation")
st.sidebar.title("üìö Table of Contents")

pages = ["Overview","Exploration","Data Visualization","Modeling","Recommendations","Conclusion"]
page  = st.sidebar.radio("Go to", pages)

# ----------------------- Overview -----------------------
if page == "Overview":
    st.header("Overview")
    st.markdown("""
    Welcome! This app presents our Movie Recommendation System project.
    
    #### Table of Topics:
    - üìå Introduction to the Movie Recommendation System  
    - üìä Overview of the Dataset  
    - ü§ñ Models used: Content-Based, Collaborative & Hybrid  
    - üé• Example Movie Recommendations  
    - üöÄ Future Improvement Ideas  
    - ‚úÖ Conclusion  
    """)

# ----------------------- Exploration -----------------------
elif page == "Exploration":
    st.header("üîç Data Exploration")
    st.subheader("Sample: Movies")
    st.dataframe(movies.head(10))

    st.subheader("Sample: Ratings")
    st.dataframe(ratings.head(10))
    
    st.subheader("Statistics: Ratings per User")
    user_rating_counts = ratings.groupby('userId')['rating'].count()
    st.write(user_rating_counts.describe())

    st.subheader("Statistics: Ratings per Movie")
    movie_rating_counts = ratings.groupby('movieId')['rating'].count()
    st.write(movie_rating_counts.describe())

# ----------------------- Data Visualization -----------------------
elif page == "Data Visualization":
    st.header("üìä Data Visualizations")

    st.subheader("Distribution of Movie Ratings")
    st.image("data/rating_distribution.png", caption="Distribution of Movie Ratings", use_container_width=True)

    st.subheader("Number of Movies per Genre")
    st.image("data/number_of_movies_per_genre.png", caption="Number of Movies per Genre", use_container_width=True)

    st.subheader("Average Movie Rating by Genre")
    st.image("data/average_rating_per_genre.png", caption="Average Movie Rating by Genre", use_container_width=True)

    st.subheader("User Activity Distribution")
    st.image("data/user_activity_distribution.png", caption="User Activity Distribution", use_container_width=True)

# ----------------------- Modeling -----------------------
elif page == "Modeling":
    st.header("ü§ñ Models Used")

    st.subheader("Content-Based Filtering")
    st.markdown("""
    **Core Functionality**  
    Recommends movies based on thematic similarity using genre signatures via TF-IDF vectorization and cosine similarity.

    **Key Features**:
    - Processes genre tags (e.g., "Action|Adventure") into weighted vectors
    - Identifies nearest neighbors in genre-space (top 10 similar films)
    - Validation showed 96% of recommendations exceed 0.85 similarity threshold

    **Strengths**:
    - Solves item cold-start (new/unrated movies)
    - Transparent logic (explainable through shared genres)
    - 40% more memory-efficient than collaborative alternatives

    **Limitations**:
    - Cannot personalize for new users (user cold-start persists)
    - Oversimplification for broadly tagged films (e.g., dramas)
    - Perfect 1.0 similarity scores indicate potential tag standardization issues
    """)

    # Collaborative Filtering Section
    st.subheader("Collaborative Filtering")

    st.markdown("""
    Collaborative Filtering is a popular recommendation technique that predicts a user‚Äôs interests by collecting preferences from many users. It is particularly useful when item metadata is sparse, as it relies solely on user-item interactions (ratings).

    We explored two memory-based collaborative filtering approaches:
    - **User-Based Filtering**: Recommends items based on the preferences of similar users.
    - **Item-Based Filtering**: Recommends items that are similar to what the user has already liked.

    This approach leverages collective intelligence and avoids hand-crafted content features.
    """)

    # --- User-Based Filtering ---
    st.markdown("### User-Based Collaborative Filtering")

    st.markdown("""
    This model assumes that users who rated the same items similarly in the past will continue to have similar preferences in the future. To calculate similarity, we used **Cosine Similarity** on a sampled subset of 5,000 users due to performance constraints. The matrix was converted into a **CSR format** for memory efficiency.

    We evaluated the model using 2-fold cross-validation with the Surprise library.
    """)

    st.code("""
    from surprise import KNNBasic
    sim_options = {'name': 'cosine', 'user_based': True}
    algo = KNNBasic(sim_options=sim_options)
    """, language="python")

    # Show evaluation results
    st.write("**Model Performance (User-Based CF):**")
    st.write("- RMSE: **1.0093**")
    st.write("- MAE: **0.7666**")

    # --- Item-Based Filtering ---
    st.markdown("### Item-Based Collaborative Filtering")

    st.markdown("""
    This approach assumes that items rated similarly by many users are themselves similar. Recommendations are generated based on item-to-item similarity. We implemented this both manually and using Surprise with Cosine Similarity.

    The item-based model demonstrated better stability and accuracy due to less sparsity in item co-ratings.
    """)

    st.code("""
    from surprise import KNNBasic
    sim_options = {'name': 'cosine', 'user_based': False}
    algo = KNNBasic(sim_options=sim_options)
    """, language="python")

    st.write("**Model Performance (Item-Based CF):**")
    st.write("- RMSE: **0.9751**")
    st.write("- MAE: **0.7445**")

  
    # --- Reflections and Next Steps ---

    st.markdown("### Reflections & Limitations")
    st.markdown("""
    While memory-based collaborative filtering helped us establish strong baselines, it presented some limitations:
    - **Sparse Matrix Problem**: Many users rate only a few items, making similarity computation less reliable.
    - **Cold Start Problem**: Difficult to recommend for new users or new items.
    - **Scalability**: Computing pairwise similarities across users or items becomes inefficient for large datasets.

    These challenges led us to explore more advanced, model-based techniques such as **Hybrid Models**, combining collaborative and content-based filtering for improved performance and flexibility.
    """)

    st.subheader("Hybrid Model")

    # --- SVD Model Description ---
    st.subheader("SVD (Singular Value Decomposition) ‚Äì Hyperparameter Optimization")

    st.markdown("""
    For the SVD model, we performed a **GridSearchCV** to tune various hyperparameters. These parameters included:
    - **n_factors**: The number of latent factors to consider.
    - **lr_all**: The learning rate.
    - **reg_all**: The regularization parameter.

    We tested different combinations of these parameters using a parameter grid, as shown below:
    ```python
    param_grid_svd = {
        'n_factors': [50, 100],
        'lr_all': [0.005, 0.01],
        'reg_all': [0.02, 0.1]
    }
        After running the grid search, the best-performing SVD configuration resulted in the following parameters:

    Best Parameters: {'n_factors': 100, 'lr_all': 0.01, 'reg_all': 0.1}
    Best RMSE: 0.9759
    This configuration suggests that a larger number of latent factors (100), combined with a learning rate of 0.01 and regularization parameter of 0.1, provided the best balance between model complexity and prediction accuracy for this dataset.
    """)

    st.subheader("KNN (User-based Collaborative Filtering) ‚Äì Hyperparameter Optimization")
    st.markdown(""" Similarly, for the KNN model, we optimized the k parameter (number of neighbors) and the similarity measure (cosine vs. pearson). 
    The parameter grid we used is as follows:

    param_grid_knn = {
        'k': [20, 40, 60],
        'sim_options': {
            'name': ['cosine', 'pearson'],
            'user_based': [True]
        }
    }
    Through the grid search, we found the best-performing parameters for the KNN model to be:

    Best Parameters: {'k': 60, 'sim_options': {'name': 'pearson', 'user_based': True}}
    Best RMSE: 0.9707
    These results suggest that using Pearson similarity and a larger number of neighbors (k = 60) helped the model achieve better performance in terms of RMSE.
    """)

    # --- Model Comparison ---
    st.subheader("Model Comparison and Insights")

    st.markdown("""
    After tuning the hyperparameters for both SVD and KNN, we observed that while both models performed well, the KNN model slightly outperformed the SVD model in terms of RMSE. Specifically:

    SVD RMSE: 0.9759
    KNN RMSE: 0.9707
    These results suggest that KNN may be more effective for this dataset, given its ability to capture user-user similarity through the user-based collaborative filtering approach.

    However, both models are capable of generating accurate recommendations. To further enhance performance, we combined the strengths of both models in a hybrid model, which averages the predictions of SVD and KNN.
    """)

   # --- Hybrid Model Description ---

    st.subheader("Hybrid Model ‚Äì Combining SVD and KNN")

    st.markdown("""
    The Hybrid Model combines the predictions of both SVD and KNN to improve recommendation accuracy. By taking the average of the two models' predictions, we aim to leverage the complementary strengths of both models:

    SVD: A matrix factorization technique that is effective in capturing latent factors for both users and items.
    KNN: A memory-based technique that computes user or item similarity, making it suitable for capturing local patterns.
    We first calculate predictions using both models for each user-item pair and then compute the hybrid score by averaging the predictions of SVD and KNN. If the KNN model cannot provide a prediction for a particular user-item pair (due to missing similarity), we fall back on the SVD model's prediction.

    Hybrid Model Evaluation:
    We evaluated the performance of the hybrid model by comparing the RMSE and MAE with the individual models (SVD and KNN). The hybrid approach typically shows improved performance due to its ability to combine the strengths of both models.
    
    Conclusion:
    The combination of SVD and KNN through the hybrid model provides a balanced approach to recommendation generation. By merging the latent factor-based SVD and the similarity-based KNN, the hybrid model offers the ability to handle both dense and sparse regions in the data, improving the overall recommendation performance.
    """)

  # ‚Äî‚Äî‚Äî Performance Metrics ‚Äî‚Äî‚Äî
    st.markdown("**Model Performance Comparison**")
    metrics = pd.DataFrame({
        "Model": ["KNN", "SVD", "Hybrid"],
        "RMSE":  [0.9187, 0.8116, 0.8246],
        "MAE":   [0.6896, 0.6152, 0.6248],
    })
    st.table(metrics)

    # ‚Äî‚Äî‚Äî GridSearchCV & Sampling Details ‚Äî‚Äî‚Äî
    st.markdown("**Hyperparameter Tuning & Sampling**")
    details = pd.DataFrame({
        "Procedure": ["SVD GridSearchCV", "KNN Subsample", "SVD Final Sample"],
        "Specs": [
            "GridSearchCV on 100k ratings (3-fold): n_factors=[50,100,150], n_epochs=[30,50], lr_all=[0.001,0.005,0.01], reg_all=[0.001,0.01,0.1]",
            "Top 5k users (full trainset) for KNN similarity",
            "1 000 000 random ratings for final SVD training"
        ]
    })
    st.table(details)

    # ‚Äî‚Äî‚Äî Interpretation Tools ‚Äî‚Äî‚Äî
    st.markdown("**Interpretation & Error Analysis**")
    st.markdown("""
    - **Precision@10 / Recall@10**: Evaluation of top-10 recommendations (Precision@10=0.8343, Recall@10=0.5725)  
    - **Error by User Activity**: Quartiles (‚Äòcold‚Äô ‚Üí ‚Äòhot‚Äô) show mean absolute error dropping from 0.665 ‚Üí 0.619  
    - **Latent Factor Inspection**: SVD factors for a sample movie/user to illustrate what the model ‚Äòlearns‚Äô  
    - **KNN Similarity Inspection**: Top-5 similar users and similarity scores for a target user
    """)

# ----------------------- Recommendations -----------------------
elif page == "Recommendations":
    st.header("üé• Example Recommendations")

    st.subheader("Content-Based Filtering Recommendations")
    # 1. Example Recommendation using Content-Based Filtering
    # Content-Based Filtering Section

    @st.cache_resource
    def load_content_based_models():
        data = {
            'cosine_sim': joblib.load("data/content_cosine_sim.pkl"),
            'movie_indices': joblib.load("data/content_movie_indices.pkl"),
            'movies': pd.read_csv("data/movies.csv")
        }
        return data

    cb_data = load_content_based_models()
    cosine_sim = cb_data['cosine_sim']
    indices = cb_data['movie_indices']
    movies = cb_data['movies']

   # Recommendation Function
    @st.cache_data
    def get_recommendations(title, top_n=10):
        idx = indices[title]
        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:top_n+1]
        movie_indices = [i[0] for i in sim_scores]
        return movies.iloc[movie_indices][['title', 'genres']]

    # UI Components
    st.subheader("üé¨ Content-Based Recommendations")
    selected_movie = st.selectbox(
        "Select a movie to get similar recommendations:",
        movies['title'].sample(100).sort_values().tolist()
    )

    if st.button("Get Recommendations (Content-Based)"):
        with st.spinner('Finding similar movies...'):
            recommendations = get_recommendations(selected_movie)
            
            st.success("Top 10 Similar Movies")
            st.dataframe(
                recommendations,
                column_config={
                    "title": "Recommended Movie",
                    "genres": "Genres"
                },
                hide_index=True
            )
            
            # Display metrics
            current_genres = set(movies[movies['title'] == selected_movie]['genres'].iloc[0].split('|'))
            shared_genres = []
            for rec in recommendations['genres']:
                rec_genres = set(rec.split('|'))
                shared_genres.append(len(current_genres.intersection(rec_genres)))
            
            avg_shared = np.mean(shared_genres)
            st.caption(f"Average shared genres: {avg_shared:.1f}/5")
            st.caption(f"Similarity confidence: >0.85 for {sum(s > 0.85 for s in shared_genres)/10:.0%} of recommendations")

    # Key Metrics Display
    st.subheader("üìä Model Performance")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Genre Consistency (Animation)", "4.5/5")
    with col2: 
        st.metric("Genre Consistency (Drama)", "2.0/5")

    st.progress(0.96, text="Global similarity >0.85 threshold")

     #Collaborative Filtering
    # ---------------------------------------------
    # üì• Daten & Modelle laden (nur 1x durch Caching)
    # ---------------------------------------------
    @st.cache_data
    def load_data():
        ratings = pd.read_csv("data/ratings.csv")
        movies = pd.read_csv("data/movies.csv")
        return ratings, movies

    @st.cache_resource
    def load_models_and_prepare():
        ratings, movies = load_data()

        # Sample User-IDs
        sample_user_ids = ratings['userId'].drop_duplicates().sample(5000, random_state=42)
        sampled_ratings = ratings[ratings['userId'].isin(sample_user_ids)].copy()

        # Mapping (kann n√ºtzlich f√ºr sp√§tere Visualisierung sein)
        user_id_map = {old: new for new, old in enumerate(sampled_ratings['userId'].unique())}
        movie_id_map = {old: new for new, old in enumerate(sampled_ratings['movieId'].unique())}
        sampled_ratings['user_idx'] = sampled_ratings['userId'].map(user_id_map)
        sampled_ratings['movie_idx'] = sampled_ratings['movieId'].map(movie_id_map)

        # Surprise Trainset erstellen
        reader = Reader(rating_scale=(0.5, 5.0))
        data = Dataset.load_from_df(sampled_ratings[['userId', 'movieId', 'rating']], reader)
        trainset = data.build_full_trainset()

        # Modell laden & Trainset zuweisen
        knn_user_model = joblib.load("data/knn_user_model.pkl")
        knn_user_model.trainset = trainset

        return sampled_ratings, movies, knn_user_model

# ---------------------------------------------
# Streamlit Oberfl√§che
# ---------------------------------------------
    st.subheader("üé¨ User-Based Collaborative Recommendations")

    # Daten & Modell laden
    sampled_ratings, movies, knn_user_model = load_models_and_prepare()

    # Button f√ºr Random-User
    if st.button("üîÄ Recommend Movies for Random User"):
        # Zuf√§llige User-ID ausw√§hlen
        random_user_id = sampled_ratings['userId'].sample(1).values[0]
        st.info(f"Random User ID: {random_user_id}")

        # Filme, die der User bereits bewertet hat
        user_rated_movie_ids = sampled_ratings[sampled_ratings['userId'] == random_user_id]['movieId'].tolist()
        all_movie_ids = sampled_ratings['movieId'].unique()
        unseen_movies = [mid for mid in all_movie_ids if mid not in user_rated_movie_ids]

        # Vorhersagen berechnen
        predictions = []
        for movie_id in unseen_movies:
            try:
                pred = knn_user_model.predict(uid=random_user_id, iid=movie_id)
                predictions.append((movie_id, pred.est))
            except:
                continue

        # Top 10 Empfehlungen
        if predictions:
            top_10 = sorted(predictions, key=lambda x: x[1], reverse=True)[:10]
            top_df = pd.DataFrame(top_10, columns=['movieId', 'predicted_rating'])
            top_df = top_df.merge(movies, on='movieId')[['title', 'predicted_rating']]

            st.success("Top 10 Movie Recommendations")
            st.dataframe(top_df.sort_values(by="predicted_rating", ascending=False), hide_index=True)
        else:
            st.error("Keine Empfehlungen konnten generiert werden.")
          
 # ----------------------------------------------------------
    # üé¨ Item-Based Collaborative Recommendations
    # ----------------------------------------------------------
    st.subheader("üé¨ Item-Based Collaborative Recommendations")

    # üîÑ Daten & Modell laden (wird durch Caching nur einmal gemacht)
    @st.cache_resource
    def load_item_model():
        ratings, movies = load_data()

        # Sample User-IDs
        sample_user_ids = ratings['userId'].drop_duplicates().sample(5000, random_state=42)
        sampled_ratings = ratings[ratings['userId'].isin(sample_user_ids)].copy()

        # ID Mapping (optional)
        user_id_map = {old: new for new, old in enumerate(sampled_ratings['userId'].unique())}
        movie_id_map = {old: new for new, old in enumerate(sampled_ratings['movieId'].unique())}
        sampled_ratings['user_idx'] = sampled_ratings['userId'].map(user_id_map)
        sampled_ratings['movie_idx'] = sampled_ratings['movieId'].map(movie_id_map)

        # Surprise Dataset
        reader = Reader(rating_scale=(0.5, 5.0))
        data = Dataset.load_from_df(sampled_ratings[['userId', 'movieId', 'rating']], reader)
        trainset = data.build_full_trainset()

        # Modell laden
        knn_item_model = joblib.load("data/knn_item_model.pkl")
        knn_item_model.trainset = trainset

        return sampled_ratings, movies, knn_item_model

    # üì¶ Daten & Modell laden
    sampled_ratings_item, movies_item, knn_item_model = load_item_model()

    # Button f√ºr zuf√§lligen User
    if st.button("üîÄ Recommend Movies for Random User (Item-Based)"):
        random_user_id = sampled_ratings_item['userId'].sample(1).values[0]
        st.info(f"Random User ID: {random_user_id}")

        # Bewertete & unbewertete Filme
        user_rated_movie_ids = sampled_ratings_item[sampled_ratings_item['userId'] == random_user_id]['movieId'].tolist()
        all_movie_ids = sampled_ratings_item['movieId'].unique()
        unseen_movies = [mid for mid in all_movie_ids if mid not in user_rated_movie_ids]

        # Vorhersagen berechnen
        predictions = []
        for movie_id in unseen_movies:
            try:
                pred = knn_item_model.predict(uid=random_user_id, iid=movie_id)
                predictions.append((movie_id, pred.est))
            except:
                continue

        # Top 10 Empfehlungen anzeigen
        if predictions:
            top_10 = sorted(predictions, key=lambda x: x[1], reverse=True)[:10]
            top_df = pd.DataFrame(top_10, columns=['movieId', 'predicted_rating'])
            top_df = top_df.merge(movies_item, on='movieId')[['title', 'predicted_rating']]

            st.success("Top 10 Movie Recommendations (Item-Based)")
            st.dataframe(top_df.sort_values(by="predicted_rating", ascending=False), hide_index=True)
        else:
            st.error("Keine Empfehlungen verf√ºgbar.")



 # Load precomputed models
    @st.cache_resource
    def load_models():
        data = joblib.load("data/precomputed_models_full.pkl")
        return data['algo_svd'], data['algo_knn'], data['ratings_filtered']

    algo_svd, algo_knn, ratings_filtered = load_models()

    user_ids = ratings_filtered['userId'].unique()
    target_user = st.selectbox("Select a user ID to get recommendations:", user_ids)

    # Predict for unseen movies
    user_rated = set(ratings_filtered[ratings_filtered['userId'] == target_user]['movieId'])
    all_movies = set(ratings_filtered['movieId'].unique())
    unseen = all_movies - user_rated

    hybrid_recommendations = []
    for movie_id in unseen:
        try:
            pred_knn = algo_knn.predict(target_user, movie_id).est
        except:
            pred_knn = None
        try:
            pred_svd = algo_svd.predict(target_user, movie_id).est
        except:
            continue

        score = (0.3 * pred_knn + 0.7 * pred_svd) if pred_knn else pred_svd
        hybrid_recommendations.append((movie_id, score))

    # Top 10
    top_recs = sorted(hybrid_recommendations, key=lambda x: x[1], reverse=True)[:10]
    rec_df = pd.DataFrame(top_recs, columns=["movieId", "Predicted Rating"]).merge(movies[['movieId', 'title']], on='movieId')

    st.subheader("üì¢ Top 10 Recommendations (Hybrid)")
    st.table(rec_df[['title', 'Predicted Rating']])
  

# ----------------------- Conclusion -----------------------
elif page == "Conclusion":
    st.header("‚úÖ Conclusion")
    st.markdown("""
    We presented a **hybrid recommender system** designed to:
    - **Compensate** for the weaknesses of purely content-based (e.g., low variance) and collaborative filtering methods  
      (e.g., cold-start, sparsity issues) (Bodduluri et al., 2024)  
    - **Create business value** by improving recommendation accuracy and increasing user engagement (Jannach & Jugovac, 2019)
    
    **Scientific References:**
    1. Bodduluri, K. C. et al. (2024). _Exploring the Landscape of Hybrid Rec. Systems in E-Commerce._ IEEE Access.  
    2. Jannach, D. & Jugovac, M. (2019). _Measuring the business value of recommender systems._ ACM TMIS.  
    3. Milvus (2025). ‚ÄúWhat defines a hybrid recommender system‚Ä¶‚Äù  
    4. Zilliz (2025). ‚ÄúHow do you handle noisy data‚Ä¶‚Äù  
    5. Ghanem, N. et al. (2022). _Balancing consumer and business value‚Ä¶_  
    6. Khorshidi, I. & Ghaffari, A. (2017). _A hybrid recommender system‚Ä¶_  
    7. Mazlan, I. et al. (2023). _Exploring the impact of hybrid recommender systems‚Ä¶_
    
    **Business Takeaways:**
    - Higher **user retention** due to more relevant and personalized recommendations  
    - **Scalability**: Hybrid approaches are modular and easily extendable (e.g., with content, context, or behavior features)  
    - **ROI impact**: Even small improvements in RMSE can result in significantly more clicks and conversions (Jannach & Jugovac, 2019)
    
    **Next Steps:**
    - Integration of **real-time user feedback loops**  
    - Extensions using **deep learning** (e.g., autoencoders, transformer-based embeddings)  
    - **Live A/B testing** to directly evaluate impact on business KPIs
    """)
